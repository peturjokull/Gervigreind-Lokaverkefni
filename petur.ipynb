{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pakkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import string\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Dropout, Flatten, Input, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adadelta\n",
    "from tensorflow.python.keras.layers import Conv1D, MaxPooling1D, GRU\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Innlestur gagna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv',delimiter=',')\n",
    "\n",
    "test_data = pd.read_csv('data/test.csv',delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gagnasettið\n",
    "Hér má sjá gagnasettið sem við þjálfum módelið á. Við erum með comment_text og hann er búið að greina (0 eða 1) hvort hann sé toxic, severe_toxic, obscene, threat, insult eða identity_hate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dálkarnir eru: \n",
      "- \t id\n",
      "- \t comment_text\n",
      "- \t toxic\n",
      "- \t severe_toxic\n",
      "- \t obscene\n",
      "- \t threat\n",
      "- \t insult\n",
      "- \t identity_hate\n"
     ]
    }
   ],
   "source": [
    "display(train_data.head())\n",
    "cols = train_data.columns\n",
    "print('Dálkarnir eru: ')\n",
    "for col in cols:\n",
    "    print('- \\t',col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def textpreprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    #Taka út ensk stopporð\n",
    "    stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "    for sw in stopwords:\n",
    "        if len(sw) > 1 and sw in sentence:\n",
    "            sentence.replace(sw,'')\n",
    "    \n",
    "    #Taka út tölur\n",
    "    sentence = ''.join([i for i in sentence if not i.isdigit()])\n",
    "    \n",
    "    #Taka út punkta, kommur og þannig lagað\n",
    "    sentence=sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drasl = train_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hér er þá búið að preprocessa textann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProcessedText(dataset):\n",
    "    x_train = dataset.to_numpy()\n",
    "    for i in range(len(x_train)):\n",
    "        x_train[i] = textpreprocess(x_train[i])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['comment_text'].to_numpy()\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = textpreprocess(x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comment_length = len(train_data['comment_text'][0])\n",
    "\n",
    "for comment in train_data['comment_text']:\n",
    "    if len(comment) > max_comment_length:\n",
    "        max_comment_length = len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubet(dataset, size):\n",
    "    return dataset[0:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prófa að exclude-a bara toxic dálkinn og gera model á honum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicCol = train_data['toxic']\n",
    "classes = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571 159571\n",
      "=================\n",
      "| train_data shape: (127656,)\n",
      "| 127656 train samples\n",
      "| 31915 test samples\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "# X_train = train_data['comment_text'].to_numpy()\n",
    "X_train = temp\n",
    "# y_train = (train_data.values[:,2:])\n",
    "y_train = train_data[classes]\n",
    "print(len(y_train), len(X_train))\n",
    "# Set aside validation data for monitoring training progress\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "print('=================')\n",
    "print('|','train_data shape:', X_train.shape)\n",
    "print('|',X_train.shape[0], 'train samples')\n",
    "print('|',y_test.shape[0], 'test samples')\n",
    "print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hér er gagnasettinu skipt jafn á milli toxic og non toxic commenta\n",
    "    #### skoða þetta from sklearn.metrics import classification_report\n",
    "    \n",
    "Bý hérna til þjálfunargögn með upprunalegum commentum þ.e. ekki upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = train_data['comment_text']\n",
    "original_testData = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_testData=original_testData.sample(frac=1)\n",
    "\n",
    "original_X_testData=original_testData['comment_text']\n",
    "original_Y_testData=original_testData[classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_X_testData = getProcessedText(original_X_testData)\n",
    "original_Y_testData=original_Y_testData.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = train_data[train_data.toxic==0]\n",
    "df_minority = train_data[train_data.toxic==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority=df_majority.sample(frac=1)\n",
    "df_upsampled = pd.concat([pd.DataFrame(df_majority.values[0:len(df_minority)],columns=train_data.columns)\n",
    ", df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31615</th>\n",
       "      <td>53f6348bd9e5ef6e</td>\n",
       "      <td>piss off homo \\n\\nstop i say stop it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>85a711bf9905af42</td>\n",
       "      <td>nicholas i think you are assuming abortion imp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104395</th>\n",
       "      <td>2e8d843809f2603b</td>\n",
       "      <td>without consensus who the fuck do you think yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148725</th>\n",
       "      <td>512eaff7387d2f23</td>\n",
       "      <td>republic of mosquito \\n\\nfaggot faggot get off...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>dfa110aa2e4b33d5</td>\n",
       "      <td>can anyone find a blue paul terrier picture we...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "31615   53f6348bd9e5ef6e               piss off homo \\n\\nstop i say stop it   \n",
       "640     85a711bf9905af42  nicholas i think you are assuming abortion imp...   \n",
       "104395  2e8d843809f2603b  without consensus who the fuck do you think yo...   \n",
       "148725  512eaff7387d2f23  republic of mosquito \\n\\nfaggot faggot get off...   \n",
       "4112    dfa110aa2e4b33d5  can anyone find a blue paul terrier picture we...   \n",
       "\n",
       "       toxic severe_toxic obscene threat insult identity_hate  \n",
       "31615      1            0       0      0      0             0  \n",
       "640        0            0       0      0      0             0  \n",
       "104395     1            0       1      0      1             0  \n",
       "148725     1            0       1      0      1             1  \n",
       "4112       0            0       0      0      0             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_upsampled=df_upsampled.sample(frac=1)\n",
    "display(df_upsampled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_y_train = df_upsampled[classes]\n",
    "upsampled_x_train = df_upsampled['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_x_train = getProcessedText(upsampled_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hérna er þetta 6118\n",
      "Hérna er þetta 6118\n",
      "=================\n",
      "| train_data shape: (24470,)\n",
      "| 24470 train samples\n",
      "| 6118 test samples\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "assert len(upsampled_x_train) == len(upsampled_y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(upsampled_x_train, upsampled_y_train, test_size=0.2)\n",
    "\n",
    "print('Hérna er þetta', len(X_test))\n",
    "print('Hérna er þetta', len(y_test))\n",
    "\n",
    "X_test=original_X_testData[0:len(X_test)]\n",
    "y_test=original_Y_testData[0:len(y_test)]\n",
    "\n",
    "print('=================')\n",
    "print('|','train_data shape:', X_train.shape)\n",
    "print('|',X_train.shape[0], 'train samples')\n",
    "print('|',y_test.shape[0], 'test samples')\n",
    "print('=================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000 # <- spurning um hvað þetta eigi að vera\n",
    "output_vector_len = int(df_upsampled.fillna('').astype(str).apply(lambda x:x.str.len()).mean()[1])\n",
    "tokenizer = text.Tokenizer(num_words=max_comment_length)\n",
    "num_classes = 6\n",
    "\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "tokTrain = tokenizer.texts_to_sequences(X_train)\n",
    "tokTest = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train=sequence.pad_sequences(tokTrain, maxlen=max_comment_length)\n",
    "x_test=sequence.pad_sequences(tokTest, maxlen=max_comment_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Góð útskýring á multilabel classifier\n",
    "https://medium.com/@vijayabhaskar96/multi-label-image-classification-tutorial-with-keras-imagedatagenerator-cd541f8eaf24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 92% acc. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 5000, 334)         6680000   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 5000, 334)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 5000, 32)          21408     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2500, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 1250, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1250, 32)          0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 128)               61824     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 6,776,278\n",
      "Trainable params: 6,776,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_comment_length,))\n",
    "model = Sequential()\n",
    "model = Embedding(max_words, output_vector_len)(inp) \n",
    "#Embedded útskýring:\n",
    "    #Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "######\n",
    "\n",
    "#####\n",
    "model = Dropout(0.5)(model)\n",
    "model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "model = MaxPooling1D(pool_size=2)(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "model = MaxPooling1D(pool_size=2)(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = GRU(128)(model)\n",
    "\n",
    "model = Dense(64, activation=\"relu\")(model)\n",
    "model = Dense(32, activation=\"relu\")(model)\n",
    "model = Dense(16, activation=\"relu\")(model)\n",
    "model = Dense(6, activation=\"sigmoid\")(model)\n",
    "model = Model(inputs=inp, outputs=model)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adadelta(), metrics=[tf.keras.metrics.AUC()])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       toxic severe_toxic obscene threat insult identity_hate\n",
      "102544     1            0       0      0      0             0\n",
      "1104       0            0       0      0      0             0\n",
      "82566      1            1       1      0      1             1\n",
      "31         0            0       0      0      0             0\n"
     ]
    }
   ],
   "source": [
    "#Sjá hvernig y- þjálfunargögnin líta út\n",
    "print(y_train[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22023 samples, validate on 2447 samples\n",
      "22023/22023 [==============================] - 2210s 100ms/sample - loss: 0.3417 - auc: 0.8552 - val_loss: 0.2971 - val_auc: 0.9050\n",
      "6118/6118 [==============================] - 82s 13ms/sample - loss: 0.1971 - auc: 0.9164\n",
      "Test acc: 0.9164426922798157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test acc: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6118, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test,batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47008893 0.02315657 0.12893164 0.01507867 0.1503449  0.05878903]\n",
      " [0.59849006 0.06102102 0.24003151 0.04465153 0.26126352 0.10866596]\n",
      " [0.9412674  0.14670779 0.6007785  0.08213215 0.532858   0.15021421]\n",
      " ...\n",
      " [0.9835127  0.06364855 0.6647508  0.02560893 0.5215145  0.0654731 ]\n",
      " [0.9509771  0.02470218 0.3841281  0.00876251 0.33090505 0.03751686]\n",
      " [0.87502795 0.04224735 0.3462971  0.02132561 0.32201153 0.06567137]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6118, 6)\n",
      "(6118, 6)\n"
     ]
    }
   ],
   "source": [
    "assert predictions.shape == y_test.shape\n",
    "print(predictions.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in range(predictions.shape[0]):\n",
    "    for item in range(predictions.shape[1]):\n",
    "        if predictions[line,item] > 0.5:\n",
    "            predictions[line,item] = 1\n",
    "        else:\n",
    "            predictions[line,item] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " ...\n",
      " [1 0 1 0 1 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions.astype(int))\n",
    "# x.astype(int)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6118, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6118, 6)\n",
      "0.9205350332352621\n",
      "0.07946496676473794\n"
     ]
    }
   ],
   "source": [
    "print(arr.shape)\n",
    "fjoldi = (arr.shape[0]*arr.shape[1])\n",
    "print(rett/fjoldi)\n",
    "print(vitlaust/fjoldi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_params(param, train_acc_, val_acc_, param_name, title):\n",
    "    plt.plot(param, train_acc_,label='Training accuracy')\n",
    "    plt.plot(param, val_acc_,label='Validation accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def find_best_SVM_params(Cs, gammas, kernel_, xtrain, ytrain, xval, yval):\n",
    "    training_acc = np.zeros(len(Cs))\n",
    "    val_acc = np.zeros(len(Cs))\n",
    "    # ignores the gamma values if linear\n",
    "    if kernel_ != 'linear':\n",
    "        training_acc = np.zeros((len(Cs), len(gammas)))\n",
    "        val_acc = np.zeros((len(Cs), len(gammas)))\n",
    "    \n",
    "    for i in range(len(Cs)):\n",
    "        print(\"c: \", Cs[i])\n",
    "        if kernel_ != 'linear':\n",
    "            for j in range(len(gammas)):\n",
    "                print(\"gamma: \", gammas[j], end =\" \")\n",
    "                clf = OneVsRestClassifier(SVC(gamma=gammas[j], C = Cs[i], kernel = kernel_))\n",
    "                clf.fit(xtrain, ytrain)\n",
    "                val_acc[i, j] = get_row_accuracy(yval, clf.predict(xval))\n",
    "                training_acc[i, j] = get_row_accuracy(ytrain, clf.predict(xtrain))\n",
    "            print()\n",
    "        else:\n",
    "            clf = OneVsRestClassifier(SVC(C = Cs[i], kernel = kernel_))\n",
    "            clf.fit(xtrain, ytrain)\n",
    "            val_acc[i] = get_row_accuracy(yval, clf.predict(xval))\n",
    "            training_acc[i] = get_row_accuracy(ytrain, clf.predict(xtrain))\n",
    "#     print('Train acc \\n', training_acc)\n",
    "#     print('Validation acc \\n', val_acc)\n",
    "    return training_acc, val_acc\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "def plot_heatmap(data_matrix, title, xlabel, ylabel, x_values, y_values, vmin_, midpoint_):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(data_matrix, interpolation='nearest', cmap=plt.cm.seismic,\n",
    "               norm=MidpointNormalize(vmin=vmin_, midpoint=midpoint_))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.grid(None)\n",
    "    plt.xticks(np.arange(len(x_values)), x_values, rotation=45)\n",
    "    plt.yticks(np.arange(len(y_values)), y_values)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "def plot_conf_mat(mat, cmap_=\"OrRd\" ):\n",
    "    tn, fp, fn, tp  = mat.ravel()\n",
    "    print('-'*50)\n",
    "    print(\"TN: {}\\t FP: {} \\nFN: {}\\t TP: {}\".format(tn, fp, fn, tp))\n",
    "    print(\"The sensitivity/recall is: {:.2%}\".format(tp/(tp+fn)))\n",
    "    print(\"The specificity is: {:.2%}\".format(tn/(tn+fp)))\n",
    "    print(\"The precision is: {:.2%}\".format(tp/(tp+fp)))\n",
    "    print('-'*50)\n",
    "    ax = sn.heatmap(mat, annot=True, fmt=\"d\", cmap=cmap_)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    # Lætur tölurnar vera í miðjum kassanum\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "totals_mat = confusion_matrix(y_test.astype(int)np.ravel(), y_pred_linear.ravel())\n",
    "plot_conf_mat(totals_mat)\n",
    "\n",
    "conmat = multilabel_confusion_matrix(y_test.astype(int), predictions.astype(int))\n",
    "# print(conmat)\n",
    "for mat in conmat:\n",
    "    tn, fp, fn, tp  = mat.ravel()\n",
    "    print('-'*100)\n",
    "    print(\"TN: {}, FP: {}, FN: {}, TP: {}\".format(tn, fp, fn, tp))\n",
    "    print(\"The sensitivity is: {:.3%}\".format(tp/(tp+fn)))\n",
    "    print(\"The specificity is: {:.3%}\".format(tn/(tn+fp)))\n",
    "    plot_heatmap(mat, 'Validation accuracy', 'Predicted', 'True Label', \n",
    "              (tn, fp), (fn,tp), np.min(mat), np.max(mat)-0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 5000, 330)         6600000   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 5000, 32)          21152     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 2500, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 1250, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1250, 32)          0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 128)               61824     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 6,693,702\n",
      "Trainable params: 6,693,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 22023 samples, validate on 2447 samples\n",
      "22023/22023 [==============================] - 867s 39ms/sample - loss: 0.3407 - accuracy: 0.8508 - val_loss: 0.2706 - val_accuracy: 0.8826\n",
      "6118/6118 [==============================] - 50s 8ms/sample - loss: 0.2668 - accuracy: 0.8855\n",
      "Test acc: 0.8855290412902832\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_comment_length,))\n",
    "model = Sequential()\n",
    "model = Embedding(max_words, output_vector_len)(inp) \n",
    "#Embedded útskýring:\n",
    "    #Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "######\n",
    "\n",
    "#####\n",
    "model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "model = MaxPooling1D(pool_size=2)(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(model)\n",
    "model = MaxPooling1D(pool_size=2)(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = GRU(128)(model)\n",
    "model = Dense(64, activation=\"relu\")(model)\n",
    "model = Dense(6, activation=\"sigmoid\")(model)\n",
    "model = Model(inputs=inp, outputs=model)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test acc: {}'.format(score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def teikna(history):\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.plot(val_loss)\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation','val. loss', 'loss'])\n",
    "    mynd = plt.show()\n",
    "    return mynd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHV drasl sem ég er ekki að nota atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "teikna(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
